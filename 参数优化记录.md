# 参数优化记录

> 本文档按照**实验轮次**递增更新，每轮包含：
> 0. 运行命令
> 1. 关键结果
> 2. 结果分析
> 3. 优化思路
> 4. 参数调整

---

## 实验 1  （baseline）

0. **运行命令**
```bash
python experiments/run_experiment.py --dataset assist09 --n_runs 5
```

1. **关键结果**
- Dropout 0.2，L2 1e-5
- 最佳 Val AUC = **0.8183** @ epoch 2
- EarlyStopping epoch 12，最终 Val AUC = 0.7607

2. **结果分析**
- 训练损失持续下降，验证 AUC 从 0.8183 → 0.7607（过拟合明显）

3. **优化思路**
- 增强正则：提高 dropout、增大 L2

4. **参数调整**
| 参数 | 原 | 调整后 |
|------|----|--------|
| dropout | 0.2 | **0.3** |
| l2_lambda | 1e-5 | **1e-4** |

---

## 实验 2  （dropout 0.3 + L2 1e-4）

0. **运行命令**
```bash
python experiments/run_experiment.py --dataset assist09 --n_runs 5
```

1. **关键结果**
- Dropout 0.3，L2 1e-4
- 最佳 Val AUC = **0.8205** @ epoch 2
- Val AUC 在 epoch 6 降至 0.7746
- 过拟合仍存在但略好于实验 1（下降幅度：5.7% → 5.6%）

2. **结果分析**
- 升级正则后最佳 AUC 略升（0.8183 → 0.8205）
- 过拟合速度稍放缓，但仍在 epoch 2 见顶
- 说明正则化力度仍不足 & 学习率过大

3. **优化思路**
- 再次抑制过拟合：
  1. **降低学习率** (0.001 → 0.0005)
  2. **进一步增大 dropout** (0.3 → 0.4)
  3. 适当缩短 Early-Stopping patience (10 → 7)

4. **参数调整**
| 参数 | 调整前 | 调整后 |
|------|--------|--------|
| lr_kt_pretrain | 0.001 | **0.0005** |
| lr_kt_finetune | 0.0005 | **0.00025** |
| dropout | 0.3 | **0.4** |
| patience | 10 | **7** |

---

## 实验 3  （dropout 0.4 + lr 0.0005 + patience 7）

0. **运行命令**
```bash
python experiments/run_experiment.py --dataset assist09 --n_runs 5
```

1. **关键结果**
- Dropout 0.4，L2 1e-4，lr 0.0005
- 最佳 Val AUC = **0.8146** @ epoch 2
- Val AUC 在 epoch 3 降至 0.8082
- 训练在第4个epoch被中断（用户手动停止）

2. **结果分析**
- 降低学习率和增大dropout后，**过拟合问题依旧**，模型仍然在第2个epoch达到性能顶峰，然后迅速开始下降。
- 这说明简单的参数调整已达瓶颈，需要更高级的训练策略来稳定训练过程。

3. **优化思路**
- **引入学习率Warmup**：在训练初期使用一个非常小的学习率，然后逐渐增加到设定的初始学习率。这有助于模型在训练初期找到一个更稳定的收敛方向，避免过早陷入局部最优或过拟合。
- **使用学习率衰减（Scheduler）**：在Warmup之后，如果验证集性能在一定轮次内没有提升，就自动降低学习率，进行更精细的调优。

4. **参数调整**
| 参数 | 调整前 | 调整后 |
|------|--------|--------|
| `lr_kt_pretrain` | 0.0005 | **保持 0.0005** |
| `warmup_steps` | - | **4000** (约半个epoch) |
| `lr_scheduler` | - | **ReduceLROnPlateau** |
| `lr_decay_patience` | - | **3** |
| `lr_decay_factor` | - | **0.5** |

> **TODO 实验 4**：运行以上新参数，完成后补充结果。

---

## 实验 A17-1  （ASSIST17 baseline - 速度问题）

0. **运行命令**
```bash
python experiments/run_experiment.py --dataset assist17 --n_runs 5
```

1. **配置**
- max_seq_len: 200
- batch_size: 32
- dropout: 0.2
- 硬件: RTX 3060 Laptop GPU

2. **关键结果**
- 训练速度: **3.5 小时/epoch** ❌ 极慢！
- Val AUC: 0.7630 → 0.7723 → 0.7774（稳步提升）✓
- 无过拟合现象 ✓
- 速度: 1.6-1.8 it/s

3. **结果分析**
- ASSIST17 平均序列长度 **551.7**（是ASSIST09的7倍）
- 20,546 个batch/epoch（是ASSIST09的3倍）
- 注意力机制复杂度：O(seq_len²)，200² = 40,000
- **预计 100 epochs 需要 350 小时（14.6天）** ❌ 不可接受
- **根本原因**：长序列导致注意力计算爆炸

4. **优化思路**
- **关键瓶颈**：序列长度的平方复杂度
- **解决方案**：
  1. 截断序列：200 → 100（预期提速 3-4倍）⭐⭐⭐
  2. 增大batch：32 → 64（预期提速 1.25倍）⭐⭐
  3. 减少图更新：每10 → 每50 batch（预期提速 1.1倍）⭐
  4. 减少轮数：100 → 50 epochs（先验证效果）
- **综合预期提速**：5-7倍（3.5小时 → 30-40分钟）

5. **参数调整**
| 参数 | 原值 | 调整后 | 理由 |
|------|------|--------|------|
| max_seq_len | 200 | **100** | 关键：减少注意力计算量（O(n²)）|
| batch_size | 32 | **64** | 提升GPU并行度 |
| n_epochs | 100 | **50** | 先验证50轮效果 |
| concept_update_freq | 10 | **50** | 减少图卷积计算 |

---

## 实验 A17-2  （优化后 - 待运行）

0. **运行命令**
```bash
python experiments/run_experiment.py --dataset assist17 --n_runs 5
```

1. **配置（已优化）**
- max_seq_len: **100** ⭐
- batch_size: **64** ⭐
- n_epochs: **50**
- concept_update_freq: **50** ⭐

2. **预期结果**
- 训练速度: **30-40 分钟/epoch**（提速 5-7倍）
- Batch数: **10,273/epoch**（减半）
- 总时间: **25-33 小时**（1-1.5天，可接受）
- Val AUC: **0.77-0.80**（预计略降0.01或持平）

3. **验证指标**
运行第1个epoch后，检查：
- [ ] 速度 > 4 it/s（提升 2.5倍）
- [ ] 耗时 < 45 分钟（提升 4倍）
- [ ] Val AUC ≥ 0.75（性能保持）
- [ ] GPU利用率 > 80%

> **TODO 实验 A17-2**：停止当前训练，重新运行优化版本。完成后补充实际结果。
