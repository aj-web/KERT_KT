# KER-KT模型实现思路与分析报告（基于论文严格验证）

## 一、论文核心方法梳理（严格按照论文第3章）

### 1.1 模型架构（KER-KT）
**模型全称**：Knowledge Enhanced Representation-driven Knowledge Tracing (KER-KT)  
**论文位置**：第3章 知识点强化表征驱动的知识追踪模型

#### 模块1：基于三支决策的知识点图表征（3.3节）

**3.3.1 知识点图构建**
- 使用Q-matrix构建知识点图
- 边权重 = 知识点共现次数：`w_{ij} = Σ_q I(k_i ∈ K_q ∧ k_j ∈ K_q)`
- 邻居集合：`N(i) = {j | w_{ij} > 0}`

**3.3.2 三支决策邻域划分**
- **相似度计算**：余弦相似度 `sim(k_i, k_j) = cos(e_i, e_j)`，范围[-1, 1]
- **三支划分**（基于阈值α和β，满足0 ≤ β < α ≤ 1）：
  - **正域**：`POS(i) = {j ∈ N(i) | sim(k_i, k_j) ≥ α}`（高相关性）
  - **边界域**：`BND(i) = {j ∈ N(i) | β < sim(k_i, k_j) < α}`（不确定性）
  - **负域**：`NEG(i) = {j ∈ N(i) | sim(k_i, k_j) ≤ β}`（低相关性/噪声）

**3.3.3 分域信息聚合**
- **正域聚合**：简单均值 `h_{pos} = (1/|POS|) Σ_{j∈POS} h_j`
- **边界域聚合**：注意力机制
  - 注意力权重：`α_j = softmax(e_{ij})`，其中`e_{ij} = a(Wh_i, Wh_j)`
  - 加权聚合：`h_{bnd} = Σ_{j∈BND} α_j Wh_j`
- **负域处理**：权重衰减 `h_{neg} = λ · (1/|NEG|) Σ_{j∈NEG} h_j`，λ=0.1
- **多域融合**：门控机制
  - 权重计算：`g = MLP([h_i, h_{pos}, h_{bnd}, h_{neg}])`，Softmax归一化
  - 融合：`h_i' = Σ_{r∈{self,pos,bnd,neg}} g_r · h_r`

**3.3.4 多层图传播**
- 第l层：`h_i^{(l)} = AGGREGATE(h_i^{(l-1)}, N(i))`
- 层间融合：`h_i = Σ_{l=1}^L w_l · h_i^{(l)}`，权重可学习，Softmax归一化
- 默认L=2层

#### 模块2：强化学习驱动的表征自适应优化（3.4节）

**3.4.1 强化学习问题建模**
- **状态空间**：`s_t = [h_t, α_t, β_t, stats_t]`
  - `h_t`：LSTM在t时刻的隐藏状态（编码学生知识掌握情况）
  - `α_t, β_t`：当前阈值参数
  - `stats_t`：三域平均节点数 `[|POS|_avg, |BND|_avg, |NEG|_avg]`
- **动作空间**：离散动作，`a_t = [Δα, Δβ]`，每个调整量可取K个值
- **阈值更新**：`α_{t+1} = clip(α_t + Δα, [α_min, 1])`，`β_{t+1} = clip(β_t + Δβ, [0, β_max])`
  - 约束：`α > β`，最小间隔防止边界域消失
- **奖励函数**（三部分加权）：
  - 准确性奖励：`R_acc = AUC_val(t)`（验证集AUC）
  - 平衡性奖励：`R_bal = -std([|POS|, |BND|, |NEG|])`（避免极端分布）
  - 稳定性奖励：`R_stab = -|Δα| - |Δβ|`（抑制剧烈波动）
  - 总奖励：`R = λ_1·R_acc + λ_2·R_bal + λ_3·R_stab`

**3.4.2 Actor-Critic框架**
- **Actor网络**：3层MLP，`π(a|s) = softmax(MLP(s))`，输出K×K种动作的概率
- **Critic网络**：3层MLP，`V(s) = MLP(s)`，输出状态价值
- **训练时**：随机采样动作；**推理时**：选择概率最高动作

**3.4.3 优化算法**
- **Critic（TD学习）**：
  - TD误差：`δ_t = r_{t+1} + γV(s_{t+1}) - V(s_t)`
  - 损失：`L_critic = δ_t^2`
  - 更新：`φ ← φ - α_c · ∇L_critic`
- **Actor（策略梯度）**：
  - 更新：`θ ← θ + α_a · δ_t · ∇log π(a_t|s_t)`
  - 当δ>0时增大该动作概率，δ<0时减小

#### 模块3：知识状态更新与预测（3.5节）

**3.5.1 LSTM序列建模**
- **交互特征编码**：`x_t = [e_q, e_k, a_t]`（题目嵌入 + 知识点增强表征 + 答题结果）
- **LSTM更新**：`h_t, c_t = LSTM(x_t, h_{t-1}, c_{t-1})`
- `h_t`编码学生在t时刻的知识掌握情况

**3.5.2 注意力机制**
- **注意力权重**：`α_i = softmax(score(h_i, h_t))`
- **上下文向量**：`c_t = Σ_i α_i · h_i`

**3.5.3 答题结果预测**
- **特征融合**：`f = [h_t, c_t, e_{q_{t+1}}, e_{k_{t+1}}]`
  - 当前知识状态 + 注意力上下文 + 目标题目嵌入 + 目标知识点增强表征
- **概率预测**：`p = sigmoid(MLP(f))`，MLP为3层结构

### 1.2 训练策略（3.6节）

**3.6.1 损失函数**
- **知识追踪损失**：`L_KT = BCE(p, y)`（二元交叉熵）
- **总损失**：`L_total = L_KT + λ_RL · L_RL`，λ_RL=0.1（默认）

**3.6.2 两阶段训练策略**
- **阶段一（Epochs 1-50）**：知识追踪预训练
  - 固定阈值α和β
  - 仅优化KT相关参数（嵌入、图传播、LSTM、注意力、MLP）
  - 优化器：Adam，学习率lr_kt=0.001
- **阶段二（Epochs 51-100）**：强化学习微调
  - 启用Actor-Critic，允许阈值动态调整
  - 联合优化所有参数
  - 多优化器策略：
    - KT部分：lr_kt=0.0005（降低）
    - Actor和Critic：lr_rl=0.0001
  - 早停：验证集AUC连续10轮无提升时停止

### 1.3 实验设置（第4章）

**4.1 数据集**
- ASSIST09：4,151学生，124知识点，325,637交互
- ASSIST17：1,709学生，102知识点，942,816交互
- Junyi：10,000+学生，835知识点，10,622,631交互

**4.2 数据划分**
- **严格时序划分**：对每个学生，前70%→训练，中间10%→验证，最后20%→测试
- 确保不利用"未来"信息

**4.2.3 参数设置**（表4.4）
- ASSIST09：d_k=128, d_q=128, d_h=256, L=2, α=0.7, β=0.3
- ASSIST17：d_k=128, d_q=128, d_h=256, L=2, α=0.7, β=0.3
- Junyi：d_k=256, d_q=256, d_h=512, L=3, α=0.65, β=0.35
- 其他：λ=0.1, γ=0.99, λ_RL=0.1, batch_size=32/32/64, dropout=0.2/0.2/0.3

**4.3 基线模型**
- DKT, DKVMN, SAKT, AKT, GKT（5个）

**4.3.2 评估指标**
- AUC（Area Under ROC Curve）
- ACC（Accuracy）
- 每个模型运行5次，报告均值±标准差

---

## 二、代码实现状态

### 2.1 已实现模块（简要）

| 模块 | 文件 | 状态 | 说明 |
|------|------|------|------|
| 三支决策图 | `triple_decision_graph.py` | ✅ 已实现 | 符合论文3.3节要求 |
| Actor-Critic | `actor_critic.py` | ⚠️ 部分实现 | 网络结构符合，奖励计算需修复 |
| KT预测器 | `kt_predictor.py` | ✅ 已实现 | 符合论文3.5节要求 |
| 主模型集成 | `kert_kt.py` | ⚠️ 部分实现 | 两阶段训练符合，部分方法需修复 |
| 数据加载 | `data_loader.py` | ⚠️ 基本实现 | 缺少时间异常过滤 |
| DKT基线 | `baselines/dkt.py` | ✅ 已实现 | 符合论文要求 |
| 实验框架 | `experiments/` | ⚠️ 框架有 | 需完善参数配置 |

### 2.2 需修复的关键问题 ❌

#### 问题1：奖励计算（论文3.4.1节）
- **文件**：`models/kert_kt.py`，`models/actor_critic.py`
- **问题**：使用模拟AUC值`np.random.normal(0.01, 0.005)`，不符合论文要求
- **论文要求**：`R_acc = AUC_val(t)`（验证集AUC）
- **修复方案**：
  1. 在`_rl_train_step`中，定期在验证集上评估模型
  2. 计算真实AUC作为准确性奖励
  3. 可每N个batch或每个epoch评估一次（避免过于频繁）

#### 问题2：区域统计计算（论文3.4.1节）
- **文件**：`models/kert_kt.py`的`_compute_region_stats`方法
- **问题**：使用固定比例模拟，不符合论文要求
- **论文要求**：`stats_t = [|POS|_avg, |BND|_avg, |NEG|_avg]`（所有知识点的三域平均节点数）
- **修复方案**：
  1. 对每个知识点，基于当前阈值α和β计算三支划分
  2. 统计每个知识点的三域节点数
  3. 计算所有知识点的平均值

#### 问题3：forward方法（论文3.3节）
- **文件**：`models/kert_kt.py`的`forward`方法
- **问题**：使用`dummy_adj`，应使用真实concept_graph
- **修复方案**：
  1. `forward`方法接收`concept_graph`参数
  2. 使用真实图结构进行三支决策聚合
  3. 确保concept_graph在训练和推理时都传入

#### 问题4：数据预处理（论文4.1节）
- **文件**：`data/data_loader.py`
- **问题**：未实现答题时间异常过滤
- **论文要求**：删除答题时间<1秒或>1小时的记录
- **修复方案**：在`preprocess_data`中添加时间过滤逻辑

### 2.3 待实现的基线模型 ❌

#### DKVMN（论文4.3.1节）
- **论文描述**：动态键值记忆网络，键矩阵存储知识点概念，值矩阵存储掌握状态
- **实现要点**：
  - 键矩阵：静态，存储知识点嵌入
  - 值矩阵：动态，存储知识掌握状态
  - 注意力机制：读写记忆
- **参考**：Zhang et al. 2017

#### SAKT（论文4.3.1节）
- **论文描述**：自注意力知识追踪，多头自注意力捕捉历史交互依赖
- **实现要点**：
  - Transformer架构
  - 位置编码
  - 多头自注意力
- **参考**：Pandey & Karypis 2019

#### AKT（论文4.3.1节）
- **论文描述**：上下文感知注意力知识追踪，结合GNN和注意力
- **实现要点**：
  - 知识点图集成
  - 上下文感知注意力

#### GKT（论文4.3.1节）
- **论文描述**：基于图的知识追踪，图卷积网络学习知识点表征
- **实现要点**：
  - 图卷积网络
  - 知识点图建模
  - 与LSTM结合
- **参考**：Nakagawa et al. 2019

**注意**：严格按照各基线模型的原始论文实现，不添加额外功能

---

## 三、任务计划（按实验时间节点）

### 🔥 实验前任务（必须完成才能开始实验）

#### P0-1：修复KER-KT核心问题（最高优先级）🔥🔥🔥

**目标**：确保KER-KT模型能正常运行并符合论文要求

1. **修复forward方法**（论文3.3节）
   - 文件：`models/kert_kt.py`
   - 修改`forward`方法接收`concept_graph`参数，移除`dummy_adj`

2. **修复区域统计计算**（论文3.4.1节）
   - 文件：`models/kert_kt.py`的`_compute_region_stats`
   - 基于实际图结构和阈值计算真实三域统计

3. **修复奖励计算**（论文3.4.1节）
   - 文件：`models/kert_kt.py`的`_rl_train_step`
   - 在验证集上计算真实AUC作为奖励

4. **完善数据预处理**（论文4.1节）
   - 文件：`data/data_loader.py`
   - 添加答题时间异常过滤（<1秒或>1小时）

#### P0-2：验证实验设置（高优先级）🔥🔥

5. **验证超参数配置**（论文表4.4）
   - 文件：`experiments/run_experiment.py`
   - 确保ASSIST09/17/Junyi的参数严格符合表4.4

6. **验证数据划分**（论文4.2.2节）
   - 确保每个学生严格时序划分：70%训练，10%验证，20%测试

7. **验证评估指标**（论文4.3.2节）
   - 确保支持AUC和ACC计算
   - 确保支持5次运行并报告均值±标准差

#### P0-3：实现基线模型（高优先级）🔥🔥

8. **实现DKVMN**（论文4.3.1节）
   - 文件：`baselines/dkvmn.py`
   - 严格按照Zhang et al. 2017实现

9. **实现SAKT**（论文4.3.1节）
   - 文件：`baselines/sakt.py`
   - 严格按照Pandey & Karypis 2019实现

10. **实现AKT**（论文4.3.1节）
    - 文件：`baselines/akt.py`

11. **实现GKT**（论文4.3.1节）
    - 文件：`baselines/gkt.py`
    - 严格按照Nakagawa et al. 2019实现

### ⚡ 实验后任务（实验运行完成后）

#### P1-1：结果分析与报告（中优先级）⚡

12. **生成结果表格**（论文表4.5格式）
    - 模型×数据集×指标（AUC/ACC）
    - 包含均值±标准差

13. **统计分析**
    - 5次运行结果的统计分析
    - 相对提升计算

#### P1-2：消融实验（如论文要求）⚡

14. **消融实验**（论文4.4节提到但未详细描述）
    - **暂不实现**，等待论文完整内容或用户明确要求

---

## 四、关键技术难点

### 4.1 RL奖励的实时计算
**难点**：在训练过程中实时计算验证集AUC  
**方案**：定期在验证集上评估（每N个batch或每个epoch），缓存结果

### 4.2 区域统计的准确计算
**难点**：对所有知识点计算三支划分并统计  
**方案**：基于当前阈值和概念图，对每个知识点计算三域节点数，然后求平均

### 4.3 大规模数据集训练效率
**难点**：Junyi数据集规模大（10M+交互）  
**方案**：GPU加速，适当调整batch_size（论文中Junyi用64）

---

## 五、核心原则

**严格按照论文实现，不过度设计**

1. **论文优先**：所有实现必须严格对应论文描述
2. **不自行扩展**：论文未明确提到的功能不添加
3. **参数严格**：超参数严格按照论文表4.4设置
4. **验证驱动**：每个模块实现后与论文描述对比验证

---

**报告更新时间**：基于论文严格验证后  
**下一步**：按P0优先级顺序执行实验前任务
